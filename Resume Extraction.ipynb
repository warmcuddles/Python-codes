{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "from tika import parser\n",
    "from spacy import displacy\n",
    "import re\n",
    "from spacy.matcher import PhraseMatcher\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "stop = stopwords.words('english')\n",
    "from nltk.corpus import wordnet\n",
    "#nltk.download('wordnet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [],
   "source": [
    "parsed = parser.from_file('Resume_1.pdf')\n",
    "meta = parsed[\"metadata\"]\n",
    "st = re.sub(' +', ' ', parsed['content'].strip())\n",
    "#st = re.sub(r\"(\\n){2,}\", \"\\n\", parsed['content'].strip())\n",
    "#st = re.sub(r\"( \\n){2,}\", \"\\n\", st)           #\\n{2,}|( \\n){2,}\", \"\\n\", parsed['content'].strip())\n",
    "st = re.sub(r'[^\\x00-\\x7F]+',' ', st)\n",
    "\n",
    "nlp = spacy.load('en_core_web_sm')\n",
    "doc = nlp(st)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_extension(dic):\n",
    "    if 'language' in dic.keys():\n",
    "        print(dic['language'])\n",
    "    if 'pdf:docinfo:creator_tool' in dic.keys():\n",
    "        print(dic['pdf:docinfo:creator_tool']) \n",
    "    if 'producer' in dic.keys():\n",
    "        print(dic['producer'])\n",
    "    if 'resourceName' in dic.keys():\n",
    "        print(dic['resourceName'])\n",
    "            \n",
    "def extract_mail(text):\n",
    "    emails = re.findall(r\"[a-zA-Z0-9\\.\\-+_]+@[a-zA-Z0-9\\.\\-+_]+\\.[A-Za-z]+\", str(text))\n",
    "    print(emails)\n",
    "    \n",
    "def extract_num(text):\n",
    "    pattern = re.compile(r'([+(]?\\d+[)\\-]?[ \\t\\r\\f\\v]*[(]?\\d{2,}[()\\-]?[ \\t\\r\\f\\v]*\\d{2,}[()\\-]?[ \\t\\r\\f\\v]*\\d*[ \\t\\r\\f\\v]*\\d*[ \\t\\r\\f\\v]*)')\n",
    "                # Understanding the above regex\n",
    "                # +91 or (91) -> [+(]? \\d+ -?\n",
    "                # Metacharacters have to be escaped with \\ outside of character classes; inside only hyphen has to be escaped\n",
    "                # hyphen has to be escaped inside the character class if you're not incidication a range\n",
    "                # General number formats are 123 456 7890 or 12345 67890 or 1234567890 or 123-456-7890, hence 3 or more digits\n",
    "                # Amendment to above - some also have (0000) 00 00 00 kind of format\n",
    "                # \\s* is any whitespace character - careful, use [ \\t\\r\\f\\v]* instead since newlines are trouble\n",
    "    match = pattern.findall(str(text))\n",
    "        # match = [re.sub(r'\\s', '', el) for el in match]\n",
    "        # Get rid of random whitespaces - helps with getting rid of 6 digits or fewer (e.g. pin codes) strings\n",
    "        # substitute the characters we don't want just for the purpose of checking\n",
    "    match = [re.sub(r'[,.]', '', el) for el in match if len(re.sub(r'[()\\-.,\\s+]', '', el))>6]\n",
    "        # Taking care of years, eg. 2001-2004 etc.\n",
    "    match = [re.sub(r'\\D$', '', el).strip() for el in match]\n",
    "        # $ matches end of string. This takes care of random trailing non-digit characters. \\D is non-digit characters\n",
    "    match = [el for el in match if len(re.sub(r'\\D','',el)) <= 15]\n",
    "        # Remove number strings that are greater than 15 digits        \n",
    "\n",
    "    try:\n",
    "        for el in list(match):\n",
    "            # Create a copy of the list since you're iterating over it\n",
    "            if len(el.split('-')) > 3: continue # Year format YYYY-MM-DD\n",
    "            for x in el.split(\"-\"):\n",
    "                try:\n",
    "                    # Error catching is necessary because of possibility of stray non-number characters\n",
    "                    # if int(re.sub(r'\\D', '', x.strip())) in range(1900, 2100):\n",
    "                    if x.strip()[-4:].isdigit():\n",
    "                        if int(x.strip()[-4:]) in range(1900, 2100):\n",
    "                            # Don't combine the two if statements to avoid a type conversion error\n",
    "                            match.remove(el)\n",
    "                except:\n",
    "                    pass\n",
    "    except:\n",
    "        pass\n",
    "    return match"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://nerdgeeklab.com/nerdgeeklab/user_response.php?test=50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Remove father name if present\n",
    "matcher = PhraseMatcher(nlp.vocab)\n",
    "phrase_list = ['father', 'father\\'s', 'fathers', 'Father', 'FATHER', 'FATHER\\'S', 'Father\\'s', 'Fathers']\n",
    "phrase_patterns = [nlp(text) for text in phrase_list]\n",
    "matcher.add('fatherParser', None, *phrase_patterns)\n",
    "matches = matcher(doc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Remove moter name if present\n",
    "matcher2 = PhraseMatcher(nlp.vocab)\n",
    "phrase_list2 = ['mother', 'mother\\'s', 'mothers', 'Mother', 'MOTHER', 'MOTHER\\'S', 'Mother\\'s', 'Mothers']\n",
    "phrase_patterns2 = [nlp(text) for text in phrase_list2]\n",
    "matcher2.add('motherParser', None, *phrase_patterns2)\n",
    "matches2 = matcher2(doc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [],
   "source": [
    "if matches2 == []:\n",
    "    pass\n",
    "else:\n",
    "    mother_match = str(doc[matches2[0][1]:]).split('\\n')[0]\n",
    "    text = str(doc).replace(mother_match, ' ')\n",
    "    doc = nlp(text)\n",
    "\n",
    "if matches == []:\n",
    "    pass\n",
    "else:\n",
    "    father_match = str(doc[matches[0][1]:]).split('\\n')[0]\n",
    "    text = str(doc).replace(father_match, ' ')\n",
    "    doc = nlp(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['abhishek', 'kumar', 'a129', 'vihar', 'mohan', 'pvt', '.cognizant', '.deloitte', '.untied', 'uhg', '.larsen', 'toubro', 'infotech', '.photography', 'etc', '.content', 'vedios', 'amoungs', 'socity', '.spid', '.spid', 'mmitr', '.spid', 'mahila', 'parmoter', 'dyal', 'b.com', 'sem', 'govt', 'govt', 'congnizant', 'fullstak', 'sfdc', 'sitecore', '-deloitte', 'sfdc', '-uhg', '-dsb', 'rights.and', 'smm', 'facebook', 'twiter', 'instagram', 'whatsapp']\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "Sentences = nltk.sent_tokenize(str(doc))\n",
    "Tokens = []\n",
    "for Sent in Sentences:\n",
    "    Tokens.append(nltk.word_tokenize(Sent)) \n",
    "Words_List = [nltk.pos_tag(Token) for Token in Tokens]\n",
    "\n",
    "Nouns_List = []\n",
    "\n",
    "for List in Words_List:\n",
    "    for Word in List:\n",
    "        #[NN.*]\n",
    "        \n",
    "        if re.match('[NN.*]', Word[1]):\n",
    "             Nouns_List.append(Word[0])\n",
    "\n",
    "Names = []\n",
    "for Nouns in Nouns_List:\n",
    "    if not wordnet.synsets(Nouns):\n",
    "        Names.append(Nouns)\n",
    "\n",
    "name =[]\n",
    "for n in Names:\n",
    "    if re.search(r\"\\w\", n):\n",
    "        name.append(n)\n",
    "name = list(map(str.lower, name))\n",
    "print(name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('Childnames.txt', 'r') as myfile:\n",
    "    data=myfile.read().replace('\\n', ' ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = data.lower()\n",
    "data = list(filter(None, data.split(\" \")))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [],
   "source": [
    "fi_names=[]\n",
    "for n in name:\n",
    "    if n in data:\n",
    "        fi_names.append(n)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['abhishek', 'kumar', 'mohan', 'dyal']"
      ]
     },
     "execution_count": 120,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fi_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc = nlp(str(doc).lower())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(17283935019124245801, 0, 1)]\n"
     ]
    }
   ],
   "source": [
    "from spacy.matcher import Matcher\n",
    "matcher3 = Matcher(nlp.vocab)\n",
    "pattern1 = [{'LOWER': fi_names[0]}]\n",
    "matcher3.add('name_extract', None, pattern1)\n",
    "found_matches = matcher3(doc)\n",
    "print(found_matches)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_name = str(doc[found_matches[0][1]:]).split('\\n')[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'abhishek kumar'"
      ]
     },
     "execution_count": 124,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_name.strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Resume_5.pdf\n"
     ]
    }
   ],
   "source": [
    "get_extension(meta)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['jatinreaper@gmail.com']\n"
     ]
    }
   ],
   "source": [
    "extract_mail(doc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['9625485336']"
      ]
     },
     "execution_count": 110,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "extract_num(doc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def text_summary():\n",
    "    pass\n",
    "    '''\n",
    "    for more analysis\n",
    "    '''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def university():\n",
    "    pass\n",
    "    '''\n",
    "    for univ details\n",
    "    '''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def skills():\n",
    "    pass\n",
    "    '''\n",
    "    skills\n",
    "    '''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Anything else??? Will work on it later"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
